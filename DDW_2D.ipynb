{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970b5597",
   "metadata": {},
   "source": [
    "## Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b831d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a843394",
   "metadata": {},
   "source": [
    "_ add to back later _\n",
    "For the final analysis, we chose to use policy responses to group the type of characteristic groups. \n",
    "**stringency_index**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae95a0",
   "metadata": {},
   "source": [
    "**Read the CSV file** owid_covid_data.csv\n",
    "extracted from: https://github.com/owid/covid-19-data/tree/master/public/data/\n",
    "\n",
    "As expected of the question, **confirmed deaths** was used as the target metric.\n",
    "Target = df[\"total_deaths_per_million\"]\n",
    "\n",
    "To determine the right set of variables for the learning phase, we had to look for independent variables that were had reduced multicollinearity data since that would not increase prediction accuracy. \n",
    "\n",
    "For the first step of cutting, we simply took all cumulative data the dataset offered for each metric used. For the second round of elimination, we chose to use a correlation heatmap offered by heatmap to provide us with a visual depiction of the relationship between variables.\n",
    "\n",
    "**First feature set**:\n",
    "- `Vaccinations: total_vaccinations and people_fully_vaccinated`\n",
    "- `Tests & positivity: positive_rate`\n",
    "- `Hospital & ICU: icu_patients and hosp_patients`\n",
    "- `Confirmed cases: total_cases`\n",
    "- `Excess Mortality: excess_mortality_cumulative_absolute`\n",
    "\n",
    "(maybe group countries based on whether they are \"first world, second world or third world\" as determined by policies?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"owid_covid_data.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_features = df[[\"total_vaccinations\", \"people_fully_vaccinated\", \"positive_rate\", \"icu_patients\", \"hosp_patients\", \"total_cases\", \"excess_mortality_cumulative_absolute\"]]\n",
    "\n",
    "sns.heatmap(df_first_features.corr(), annot=True, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe can delete first\n",
    "#plot the number of deaths to determine an ideal time period\n",
    "df_deaths = df[[\"total_deaths\", \"date\"]]\n",
    "df_deaths = df_deaths.drop_duplicates(subset=['date'])\n",
    "\n",
    "plt.scatter(df_deaths.index, df_deaths[\"total_deaths\"])\n",
    "\n",
    "# find POI = 0 (second derivative test)\n",
    "display(df_deaths)\n",
    "df_deaths[[\"first_derivative\"]] = df_deaths[\"total_deaths\"]\n",
    "df_deaths[[\"first_derivative\"]] = df_deaths[\"first_derivative\"].shift()\n",
    "df_deaths[[\"first_derivative\"]] = df_deaths[\"first_derivative\"]-df_deaths[\"total_deaths\"]\n",
    "\n",
    "df_deaths[[\"second_derivative\"]] = df_deaths[\"first_derivative\"]\n",
    "df_deaths[[\"second_derivative\"]] = df_deaths[\"second_derivative\"].shift()\n",
    "df_deaths[[\"second_derivative\"]] = df_deaths[\"second_derivative\"]-df_deaths[\"first_derivative\"]\n",
    "display(df_deaths)\n",
    "plt.scatter(df_deaths.index, df_deaths[\"first_derivative\"])\n",
    "plt.scatter(df_deaths.index, df_deaths[\"second_derivative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary data cleaning:\n",
    "df[\"time\"] = pd.to_datetime(df['dates'], format = %Y%m%d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96076c17",
   "metadata": {},
   "source": [
    "### Hypothesis 1:\n",
    "\n",
    "A initial hypothesis we make assumes that quantity of policies undertaken by a country crucially affects the number of deaths in that country. As such, we group the countries into 4 large subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ea0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_policies = df[[\"stringency_index\", \"location\"]]\n",
    "df_policies = df_policies.drop_duplicates(subset=['location'])\n",
    "\n",
    "#plt.scatter(df_policies[\"stringency_index\"], df_policies[\"location\"])\n",
    "display(df_policies.describe())\n",
    "#plt.scatter(df_policies.index, df_policies[\"stringency_index\"])\n",
    "\n",
    "divider1, divider2, divider3 = df_policies.describe().loc[\"25%\", \"50%\", \"75%\"]\n",
    "print(divider1, divider2, divider3)\n",
    "\n",
    "country_subset1 = df_policies.loc[(df_policies[\"stringency_index\"] < divider1), [\"location\"]]\n",
    "print(display(country_subset1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6df42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc2353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second_features = df[[\"people_fully_vaccinated\", \"positive_rate\", \"hosp_patients\", \"total_cases\", \"excess_mortality_cumulative_absolute\"]]\n",
    "\n",
    "sns.heatmap(df_second_features.corr(), annot=True, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57969cf",
   "metadata": {},
   "source": [
    "### Before Cleaning the data, here are some functions to be used\n",
    "**Functions to be used** These are the functions that will be used\n",
    "- `normalize_z()`\n",
    "- `get_features_targets()`\n",
    "- `prepare_feature()`\n",
    "- `prepare_target()`\n",
    "- `predict()`\n",
    "- `predict_norm()`\n",
    "- `split_data()`\n",
    "- `mean_squared_error()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311453ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_z(df):\n",
    "    dfout = (df-df.mean(axis=0))/(df.std(axis=0))\n",
    "    return dfout\n",
    "\n",
    "def get_features_targets(df, feature_names, target_names):\n",
    "    df_feature = df[feature_names]\n",
    "    df_target = df[target_names]\n",
    "    return df_feature, df_target\n",
    "\n",
    "def prepare_feature(df_feature):\n",
    "    df1 = df_feature\n",
    "    array1 = df_feature.to_numpy()\n",
    "    array2 = np.ones((array1.shape[0], 1))\n",
    "    return np.concatenate((array2, array1), axis=1) #axis 1 means side by side (column)\n",
    "    \n",
    "def prepare_target(df_target):\n",
    "    array1 = df_target.to_numpy()\n",
    "    return array1\n",
    "\n",
    "def predict(df_feature, beta):\n",
    "    df_feature = normalize_z(df_feature)\n",
    "    df_feature = prepare_feature(df_feature)\n",
    "    result = predict_norm(df_feature, beta)\n",
    "    return result\n",
    "\n",
    "def predict_norm(X, beta):\n",
    "    # yhat = X x beta\n",
    "    array = np.matmul(X, beta)\n",
    "    return array\n",
    "\n",
    "def split_data(df_feature, df_target, random_state=100, test_size=0.3):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    values = np.random.choice(df_feature.index,int(len(df_feature)*test_size), replace = False)\n",
    "    anti_values = [i for i in df_feature.index if i not in values]\n",
    "\n",
    "    df_feature_test = df_feature.loc[values,:]\n",
    "    df_feature_train = df_feature.loc[anti_values, :]\n",
    "    \n",
    "    df_target_test = df_target.loc[values,:]\n",
    "    df_target_train = df_target.loc[anti_values,:]\n",
    "    \n",
    "    return df_feature_train, df_feature_test, df_target_train, df_target_test\n",
    "\n",
    "def mean_squared_error(target, pred):\n",
    "    error = target-pred\n",
    "    \n",
    "    return (1/(target.shape[0]))* np.matmul(error.T, error)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8805f",
   "metadata": {},
   "source": [
    "Based on the second heatmap, will be extracting the following columns:\n",
    "\n",
    "**Final feature set**: takes one from each matric\n",
    "- `Vaccinations: people_fully_vaccinated`\n",
    "- `Tests & positivity: positive_rate`\n",
    "- `Hospital & ICU: hosp_patients`\n",
    "- `Confirmed cases: total_cases`\n",
    "- `Excess Mortality: excess_mortality_cumulative_absolute`\n",
    "\n",
    "features = \"people_fully_vaccinated\", \"positive_rate\", \"hosp_patients\", \"total_cases\", \"excess_mortality_cumulative_absolute\" \n",
    "\n",
    "target = \"total_deaths_per_million\"\n",
    "\n",
    "Normalize the features using z normalization.\n",
    "Plot the data using scatter plot. Use the following columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9eb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features and the target\n",
    "df_features, df_target = get_features_targets(df, [\"people_fully_vaccinated\", \"positive_rate\", \"hosp_patients\", \"total_cases\", \"excess_mortality_cumulative_absolute\"], [\"total_deaths_per_million\"])\n",
    "\n",
    "#clean the data: replace NaN with 0\n",
    "df_features = df_features.fillna(0)\n",
    "\n",
    "# Split the data set into training and test\n",
    "df_features_train, df_features_test, df_target_train, df_target_test = (split_data(df_features, df_target, random_state=100, test_size=0.3))\n",
    "\n",
    "# Normalize the features using z normalization\n",
    "df_features_train_z = normalize_z(df_features_train)\n",
    "\n",
    "# Change the features and the target to numpy array using the prepare functions\n",
    "X = prepare_feature(df_features_train_z)\n",
    "target = prepare_target(df_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ad775",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_features_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3dbfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, beta):\n",
    "    J = 0\n",
    "    error = np.matmul(X, beta) - y\n",
    "    #print((1/(2*X.shape[0])))\n",
    "    J = (1/(2*X.shape[0]))*(np.matmul(error.T, error))\n",
    "    return J\n",
    "\n",
    "def gradient_descent(X, y, beta, alpha, num_iters):\n",
    "    m = X.shape[0]\n",
    "    for i in range(num_iters):\n",
    "        yhat = np.matmul(X, beta)\n",
    "        beta = beta - ((alpha/m)*np.matmul(X.T, (yhat-y)))\n",
    "    \n",
    "    print(beta)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "beta = np.zeros((6,1))\n",
    "\n",
    "# Call the gradient_descent function\n",
    "beta = gradient_descent(X, target, beta, alpha, iterations)\n",
    "print(\"2\", beta)\n",
    "\n",
    "# call the predict() method\n",
    "pred = predict(df_features_test, beta)\n",
    "\n",
    "###\n",
    "print(pred.shape)\n",
    "print(pred.mean())\n",
    "print(pred.std())\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad7824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_feature, df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119fe68",
   "metadata": {},
   "source": [
    "### Iteration 3: (bryan's addition)\n",
    "\n",
    "Since we were doing a future prediction model, we decided to predict for day n+1 instead of for day n. Total number of deaths/ Targets of intervention is hence displaced by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01198d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
